{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger('chatbot')\n",
    "\n",
    "class ChatBot:\n",
    "    def __init__(self, intents_file='intents.json'):\n",
    "        # Download required NLTK datasets if not already downloaded\n",
    "        nltk_data_path = nltk.data.path[0]\n",
    "        if not os.path.exists(os.path.join(nltk_data_path, 'tokenizers/punkt')):\n",
    "            logger.info(\"Downloading NLTK punkt...\")\n",
    "            nltk.download(\"punkt\", quiet=True)\n",
    "        if not os.path.exists(os.path.join(nltk_data_path, 'corpora/wordnet')):\n",
    "            logger.info(\"Downloading NLTK wordnet...\")\n",
    "            nltk.download(\"wordnet\", quiet=True)\n",
    "        \n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.intents_file = intents_file\n",
    "        self.ignore_chars = [\"?\", \"!\", \".\", \",\", \";\", \":\", \"-\", \"_\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"'\", \"\\\"\"]\n",
    "        self.model_path = \"chatbot_model.h5\"\n",
    "        self.words_path = \"words.pkl\"\n",
    "        self.classes_path = \"classes.pkl\"\n",
    "        \n",
    "        # Load or prepare training data\n",
    "        self.prepare_training_data()\n",
    "    \n",
    "    def load_intents(self):\n",
    "        \"\"\"Load intents from JSON file.\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.intents_file):\n",
    "                with open(self.intents_file, 'r') as file:\n",
    "                    return json.load(file)\n",
    "            else:\n",
    "                # Use default intents if file doesn't exist\n",
    "                return {\n",
    "                    \"intents\": [\n",
    "                        {\n",
    "                            \"tag\": \"greeting\",\n",
    "                            \"patterns\": [\"Hello\", \"Hi\", \"Hey\", \"Greetings\", \"What's up?\"],\n",
    "                            \"responses\": [\"Hello!\", \"Hey there!\", \"Hi, how can I assist you?\"]\n",
    "                        },\n",
    "                        {\n",
    "                            \"tag\": \"goodbye\",\n",
    "                            \"patterns\": [\"Bye\", \"See you later\", \"Goodbye\"],\n",
    "                            \"responses\": [\"Goodbye!\", \"Take care!\", \"See you soon!\"]\n",
    "                        },\n",
    "                        {\n",
    "                            \"tag\": \"thanks\",\n",
    "                            \"patterns\": [\"Thanks\", \"Thank you\", \"Appreciate it\"],\n",
    "                            \"responses\": [\"You're welcome!\", \"No problem!\", \"Glad to help!\"]\n",
    "                        },\n",
    "                        {\n",
    "                            \"tag\": \"weather\",\n",
    "                            \"patterns\": [\"What's the weather like?\", \"Is it raining?\", \"Tell me the weather\"],\n",
    "                            \"responses\": [\"I don't have live weather updates, but you can check a weather website!\"]\n",
    "                        },\n",
    "                        {\n",
    "                            \"tag\": \"help\",\n",
    "                            \"patterns\": [\"I need help\", \"Can you assist me?\", \"Support\"],\n",
    "                            \"responses\": [\"Sure! How can I help?\", \"I'm here to assist you.\"]\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading intents: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def prepare_training_data(self):\n",
    "        \"\"\"Prepare training data from intents.\"\"\"\n",
    "        # Check if model and data already exist\n",
    "        if (os.path.exists(self.model_path) and \n",
    "            os.path.exists(self.words_path) and \n",
    "            os.path.exists(self.classes_path)):\n",
    "            logger.info(\"Loading existing model and data...\")\n",
    "            self.model = load_model(self.model_path)\n",
    "            with open(self.words_path, 'rb') as file:\n",
    "                self.words = pickle.load(file)\n",
    "            with open(self.classes_path, 'rb') as file:\n",
    "                self.classes = pickle.load(file)\n",
    "            return\n",
    "        \n",
    "        # Load intents\n",
    "        intents = self.load_intents()\n",
    "        \n",
    "        # Tokenization & Lemmatization\n",
    "        words = []\n",
    "        classes = []\n",
    "        documents = []\n",
    "        \n",
    "        logger.info(\"Processing intents data...\")\n",
    "        for intent in intents[\"intents\"]:\n",
    "            for pattern in intent[\"patterns\"]:\n",
    "                word_list = nltk.word_tokenize(pattern)\n",
    "                words.extend(word_list)\n",
    "                documents.append((word_list, intent[\"tag\"]))\n",
    "                if intent[\"tag\"] not in classes:\n",
    "                    classes.append(intent[\"tag\"])\n",
    "        \n",
    "        # Lemmatize and remove duplicates\n",
    "        words = [self.lemmatizer.lemmatize(w.lower()) for w in words if w not in self.ignore_chars]\n",
    "        words = sorted(set(words))\n",
    "        classes = sorted(set(classes))\n",
    "        \n",
    "        logger.info(f\"Found {len(documents)} documents, {len(classes)} classes, {len(words)} unique lemmatized words\")\n",
    "        \n",
    "        # Save words and classes\n",
    "        with open(self.words_path, 'wb') as file:\n",
    "            pickle.dump(words, file)\n",
    "        with open(self.classes_path, 'wb') as file:\n",
    "            pickle.dump(classes, file)\n",
    "        \n",
    "        self.words = words\n",
    "        self.classes = classes\n",
    "        \n",
    "        # Create training data\n",
    "        training = self.create_training_data(documents, words, classes)\n",
    "        \n",
    "        # Build and train model\n",
    "        self.build_model(training)\n",
    "    \n",
    "    def create_training_data(self, documents, words, classes):\n",
    "        \"\"\"Create bag of words training data.\"\"\"\n",
    "        training = []\n",
    "        output_empty = [0] * len(classes)\n",
    "        \n",
    "        for doc in documents:\n",
    "            bag = []\n",
    "            pattern_words = [self.lemmatizer.lemmatize(w.lower()) for w in doc[0]]\n",
    "            \n",
    "            for w in words:\n",
    "                bag.append(1 if w in pattern_words else 0)\n",
    "            \n",
    "            output_row = list(output_empty)\n",
    "            output_row[classes.index(doc[1])] = 1\n",
    "            training.append([bag, output_row])\n",
    "        \n",
    "        # Shuffle and convert to array\n",
    "        random.shuffle(training)\n",
    "        return np.array(training, dtype=object)\n",
    "    \n",
    "    def build_model(self, training):\n",
    "        \"\"\"Build and train the neural network model.\"\"\"\n",
    "        # Split training data\n",
    "        X = np.array([item[0] for item in training])\n",
    "        y = np.array([item[1] for item in training])\n",
    "        \n",
    "        # Split into training and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Build model\n",
    "        model = Sequential([\n",
    "            Dense(256, input_shape=(len(X_train[0]),), activation=\"relu\"),\n",
    "            Dropout(0.5),\n",
    "            Dense(128, activation=\"relu\"),\n",
    "            Dropout(0.5),\n",
    "            Dense(64, activation=\"relu\"),\n",
    "            Dropout(0.3),\n",
    "            Dense(len(self.classes), activation=\"softmax\")\n",
    "        ])\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            loss=\"categorical_crossentropy\", \n",
    "            optimizer=Adam(learning_rate=0.001), \n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "        \n",
    "        # Set up callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001),\n",
    "            ModelCheckpoint(\n",
    "                self.model_path, \n",
    "                monitor='val_accuracy', \n",
    "                verbose=1, \n",
    "                save_best_only=True, \n",
    "                mode='max'\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        logger.info(\"Training model...\")\n",
    "        history = model.fit(\n",
    "            X_train, \n",
    "            y_train, \n",
    "            epochs=300, \n",
    "            batch_size=8, \n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Model accuracy: {model.evaluate(X_val, y_val)[1]:.4f}\")\n",
    "        self.model = model\n",
    "        \n",
    "    def preprocess_input(self, sentence):\n",
    "        \"\"\"Preprocess user input for prediction.\"\"\"\n",
    "        sentence_words = nltk.word_tokenize(sentence)\n",
    "        sentence_words = [self.lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "        bag = [1 if w in sentence_words else 0 for w in self.words]\n",
    "        return np.array(bag)\n",
    "    \n",
    "    def get_response(self, user_input, confidence_threshold=0.7):\n",
    "        \"\"\"Get chatbot response for user input.\"\"\"\n",
    "        try:\n",
    "            bag = self.preprocess_input(user_input)\n",
    "            prediction = self.model.predict(np.array([bag]))[0]\n",
    "            max_index = np.argmax(prediction)\n",
    "            confidence = prediction[max_index]\n",
    "            \n",
    "            if confidence > confidence_threshold:\n",
    "                tag = self.classes[max_index]\n",
    "                intents = self.load_intents()\n",
    "                \n",
    "                for intent in intents[\"intents\"]:\n",
    "                    if intent[\"tag\"] == tag:\n",
    "                        return {\n",
    "                            \"tag\": tag,\n",
    "                            \"response\": random.choice(intent[\"responses\"]),\n",
    "                            \"confidence\": float(confidence)\n",
    "                        }\n",
    "            \n",
    "            return {\n",
    "                \"tag\": \"unknown\",\n",
    "                \"response\": \"I'm not sure how to respond to that.\",\n",
    "                \"confidence\": float(confidence)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting response: {e}\")\n",
    "            return {\n",
    "                \"tag\": \"error\",\n",
    "                \"response\": \"Sorry, I encountered an error processing your request.\",\n",
    "                \"confidence\": 0.0\n",
    "            }\n",
    "    \n",
    "    def run_interactive_chat(self):\n",
    "        \"\"\"Run an interactive chat session in the console.\"\"\"\n",
    "        print(\"\\n🤖 Chatbot: Hello! Type 'exit' to end the conversation.\")\n",
    "        \n",
    "        while True:\n",
    "            user_input = input(\"\\nYou: \")\n",
    "            if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "                print(\"🤖 Chatbot: Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            result = self.get_response(user_input)\n",
    "            print(f\"🤖 Chatbot: {result['response']} (Confidence: {result['confidence']:.2f})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create and run chatbot\n",
    "    chatbot = ChatBot()\n",
    "    chatbot.run_interactive_chat()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
